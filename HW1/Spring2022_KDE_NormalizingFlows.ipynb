{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKFtCQz_kjDr"
   },
   "source": [
    "### Importing additional relevant libraries for Part II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "auqKoJhvjEsZ",
    "outputId": "abf38d92-460e-4540-ed42-5fe7d243c830",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Scikit-learn built-in dataset generators\n",
    "from sklearn.datasets import make_moons, make_circles, make_blobs\n",
    "\n",
    "## Progress bar\n",
    "import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import Adam\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "# Traning will be done on CPU for this homework. \n",
    "# For K=4, N=1500, epochs=1000 takes < 3 mins.\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Using device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDTnZBzDg_7i"
   },
   "source": [
    "## Part II: Invertible Neural Networks (70 points) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdTZwoJzCgmC"
   },
   "source": [
    "In this part, we will take a closer look at invertible neural networks, otherwise known as *Normalizing Flows*. The most popular, current application of normalizing flows is to model datasets of images. In this part, we will implement a type of flows called *Coupling Flows* for a simplified problem of sampling from toy 2D datasets, although similar concepts (with deeper models + tricks) can be used to model images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYwDXxQzGdl-"
   },
   "source": [
    "### General Concept \n",
    "Recall that given a random vector $Z$ with a density $p_Z(\\mathbf{z})$ (e.g. Gaussian) and an invertible function $f$, the density $p_X(\\mathbf{x})$ of $X=f(Z)$ is given by:\n",
    "\n",
    "$$\n",
    "\\log p_X(\\mathbf{x}) = \\log p_Z(f^{-1}(\\mathbf{x})) + \\log{} \\left|\\det \\frac{df^{-1}(\\mathbf{x})}{d\\mathbf{x}}\\right|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wg7VBnMqh0lS"
   },
   "source": [
    "### Toy Datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_G41iT5g71OI"
   },
   "source": [
    "The provided function `sample_2d_datasets` samples from 4 different toy datasets that we will use for this part to experiment with NFs. The supported options in this function are `{'Circles', 'Moons', 'GaussiansGrid', 'GaussiansRot'}`, where for the last distribution `'GaussiansRot'`, the function supports a varying number of gaussians using the parameter `num_gaussians`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P0WweE3-pWhL"
   },
   "outputs": [],
   "source": [
    "def sample_2d_datasets(dist_type, num_samples=1000, seed=0, num_gaussians=5):\n",
    "  \"\"\"\n",
    "  function samples from simple pre-defined distributions in 2D.\n",
    "  Inputs:\n",
    "    - dist_type: str specifying the distribution to be chosen from:\n",
    "      {'Circles', 'Moons', 'GaussiansGrid', 'GaussiansRot'}\n",
    "    - num_samples: Number of samples to draw from dist_type (int).\n",
    "    - seed: Random seed integer.\n",
    "    - num_gaussians: Number of rotated gaussians if dist_type='GaussiansRot'. \n",
    "      (relevant only for dist_type='GaussiansRot', should be a keyword argument)\n",
    "  Outputs:\n",
    "    - data (np.array): array of num_samplesx2 samples from dist_type\n",
    "  \"\"\"\n",
    "  np.random.seed(seed)\n",
    "  if dist_type == 'Circles':\n",
    "    data = make_circles(num_samples, noise=.1, factor=.8, random_state=seed, shuffle=True)[0]\n",
    "  elif dist_type == 'Moons':\n",
    "    data = make_moons(num_samples, noise=.1, random_state=seed, shuffle=True)[0]\n",
    "  elif dist_type == 'GaussiansGrid':\n",
    "    centers = np.array([[0,0],[0,2],[2,0],[2,2]])\n",
    "    data = make_blobs(num_samples, centers=centers, cluster_std=.5, random_state=seed, shuffle=True)[0]\n",
    "  elif dist_type == 'GaussiansRot':\n",
    "    angles = np.linspace(0, 2 * np.pi, num_gaussians, endpoint=False)\n",
    "    centers = np.stack([2.5 * np.array([np.cos(angle), np.sin(angle)]) for angle in angles])\n",
    "    data = make_blobs(num_samples, centers=centers, cluster_std=np.sqrt(.1), random_state=seed, shuffle=True)[0]\n",
    "  else:\n",
    "    raise NotImplementedError\n",
    "  return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKSO-JC-YDK3"
   },
   "source": [
    "<img src=\"https://img.icons8.com/offices/80/000000/making-notes.png\" style=\"height:30px;display:inline\\\">**<span style=\"color:red\">Task 1</span>**. To get acquainted with this function, for each of the 4 distributions above, draw $N = 1000$ samples ${{x}_i}$. Display the drawn samples for each distribution in a separate plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = ['Circles', 'Moons', 'GaussiansGrid', 'GaussiansRot']\n",
    "samples = [sample_2d_datasets(dist_type) for dist_type in dists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig,ax = plt.subplots(1,4,figsize=(15,3))\n",
    "for i,sample in enumerate(samples):\n",
    "    ax[i].scatter(sample[:,0],sample[:,1],s=3)\n",
    "    ax[i].set_title(dists[i])\n",
    "    plt.savefig('Toy Data Samples.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Th0_cc--C4Z"
   },
   "source": [
    "For convience purposes, we wrap the function `sample_2d_datasets` with a `torch.utils.data.Dataset` class, and implement the methods `__len__` and `__getitem__` to sample batches afterward with dataloaders when we train our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5gk4wsF8uapY"
   },
   "outputs": [],
   "source": [
    "class ToyDataset(Dataset):\n",
    "  def __init__(self, dist_type, num_samples=1000, seed=0, num_gaussians=5):\n",
    "      \"\"\"\n",
    "      Wrapper around the function \"sample_2d_datasets\" to allow iterating \n",
    "      batches using a datalaoder when training our normalizing flow model.\n",
    "      \"\"\"\n",
    "      self.data = sample_2d_datasets(dist_type, num_samples, seed, num_gaussians)\n",
    "      self.num_samples = num_samples\n",
    "\n",
    "  def __len__(self):\n",
    "      return self.num_samples\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "      return torch.from_numpy(self.data[index]).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DyKXj4l-_8Q"
   },
   "source": [
    "### Coupling Layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqNHbxDMNtau"
   },
   "source": [
    "Next, we look at possible transformations to apply inside the flow, focusing on the simplest and most efficient one. A recent popular flow layer, which works well in combination with deep neural networks, is the coupling layer introduced by [Dinh et al.](https://arxiv.org/abs/1605.08803). The input $\\mathbf{x}$ is arbitrarily split into two parts, $\\mathbf{x}_{1:j}$ and $\\mathbf{x}_{j+1:d}$, of which the first remains unchanged by the flow. Yet, $\\mathbf{x}_{1:j}$ is used to parameterize the transformation for the second part, $\\mathbf{x}_{j+1:d}$. In this coupling layer, we apply an affine transformation by scaling the input by $\\mathbf{s}$ and shifting it by $\\mathbf{t}$. In other words, our transformation $\\mathbf{y}=f(\\mathbf{x})$ looks as follows:\n",
    "\n",
    "$$\\mathbf{y}_{1:j} = \\mathbf{x}_{1:j}$$\n",
    "$$\\mathbf{y}_{j+1:d} = \\mathbf{s}_{\\theta_1}(\\mathbf{x}_{1:j}) \\odot \\mathbf{x}_{j+1:d} + \\mathbf{t}_{\\theta_2}(\\mathbf{x}_{1:j})$$\n",
    "\n",
    "$\\mathbf{y}$ is the output of the flow layer, the functions $\\mathbf{s}$ and $\\mathbf{t}$ are implemented as neural networks, and the sum and multiplication are performed element-wise. Here's a block diagram that visualize the coupling layer in the form of a computation graph:\n",
    "\n",
    "<center width=\"100%\"><img src=\"https://drive.google.com/uc?id=1R_omjSvfBN4xjkJgkmkwPX10jkjz_mmC\" width=\"400px\"></center>\n",
    "\n",
    "For convience, since we train using the log-density, it is common practice to apply an exponential on the predicted scaling factor prior to multiplication with $\\mathbf{x}_{j+1:d}$, as this simplifies the calculation of the log-determinant of the Jacobian that we will derive shortly. Hence, the implemented transformation in practice is usually:\n",
    "\n",
    "$$\\mathbf{y}_{1:j} = \\mathbf{x}_{1:j}$$\n",
    "$$\\mathbf{y}_{j+1:d} = \\exp(\\mathbf{s}_{\\theta_1}(\\mathbf{x}_{1:j})) \\odot \\mathbf{x}_{j+1:d} + \\mathbf{t}_{\\theta_2}(\\mathbf{x}_{1:j})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xfojy91fNdaR"
   },
   "source": [
    "<img src=\"https://img.icons8.com/offices/80/000000/making-notes.png\" style=\"height:30px;display:inline\\\">**<span style=\"color:red\">Task 2</span>**. Write down the inverse of this layer $\\mathbf{x}=f^{-1}(\\mathbf{y})$ for some $\\mathbf{x},\\mathbf{y} \\in \\mathbb{R}^d$. Draw the inverse function $f^{-1}(\\mathbf{y})$ as a computational graph that has $\\mathbf{y}$ as input and $\\mathbf{x}$ as output.\n",
    "\n",
    "<img src=\"https://img.icons8.com/offices/80/000000/making-notes.png\" style=\"height:30px;display:inline\\\">**<span style=\"color:red\">Task 3</span>**. Write down the Jacobian of this layer $\\frac{d \\mathbf{y}}{d \\mathbf{x}}$. Do you recognize a special structure in this matrix?\n",
    "\n",
    "<img src=\"https://img.icons8.com/offices/80/000000/making-notes.png\" style=\"height:30px;display:inline\\\">**<span style=\"color:red\">Task 4</span>**. Write down the explicit expression for the *log*-determinant of the Jacobian matrix from the previous section.\n",
    "\n",
    "<img src=\"https://img.icons8.com/offices/80/000000/making-notes.png\" style=\"height:30px;display:inline\\\">**<span style=\"color:red\">Task 5</span>**. Write down the explicit expression for the *log*-determinant of the Jacobian matrix of the *inverse* function $\\frac{d \\mathbf{x}}{d \\mathbf{y}}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Task 2\n",
    "$$ x_{1:j} = y_{1:j} $$\n",
    "$$ x_{j+1:d} = (y_{j+1:d}-t_{\\theta_2}(y_{1:j})) \\odot \\exp{(-s_{\\theta_1}(y_{1:j}))}$$\n",
    "\n",
    "### Task 3\n",
    "\n",
    "\\begin{equation*}\n",
    "J = \n",
    "\\begin{pmatrix}\n",
    "I_j & 0 \\\\\n",
    "\\frac{dy_{j+1:d}}{dx^T_{1:j}} & diag(\\exp(s(x_{1:j}))\\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "We can notice that this is a lower triangular matrix.\n",
    "\n",
    "### Task 4\n",
    "\n",
    "Since the jacobian is a lower triangular matrix the determinant is just the multiplication of the diagonal elements $\\exp[\\sum_n{s(x_{1:j})_n}] $, so the log-deteminant is simply :\n",
    "$$\\sum_n{s(x_{1:j})_n}$$\n",
    "\n",
    "### Task 5\n",
    "According to the inverse function theorem, the matrix inverse of the Jacobian matrix of an invertible function is the Jacobian matrix of the inverse function, so:\n",
    "\n",
    "$$J_{f^{-1}} = J_{f}^{-1}$$\n",
    "So the log-determinant would be: \n",
    "$$ -\\sum_n{s(y_{1:j})_n}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jWRH64IJ5eW",
    "tags": []
   },
   "source": [
    "In our implementation, we will realize the splitting of variables as masking. The variables to be transformed, $\\mathbf{x}_{j+1:d}$, are masked when passing $\\mathbf{x}$ to the networks to predict the transformation parameters $\\mathbf{s}_{\\theta_1}(\\mathbf{x}_{1:j})$ and $\\mathbf{t}_{\\theta_2}(\\mathbf{x}_{1:j})$. Also, afterward when applying the transformation (don't forget to exponentiate the scaling!), we mask the parameters for $\\mathbf{x}_{1:j}$ so that we have an identity operation for those variables.\n",
    "\n",
    "For predicting the shifting and scaling parameters for our toy datasets we will be using neural networks with 3 Fully Connected layers with LeakyReLU activations in between. Additionally, for stabilization purposes, we multiply the scaling output $\\mathbf{s}_{\\theta_1}(\\mathbf{x}_{1:j})$ prior to exponentiation with a learnable parameter per dimension `scale_factor` initialized to 0. Meaning, our scaling is initialized to 1 as $\\exp(0) = 1$. This prevents sudden large scaling values that can destabilize training (especially in the beginning).\n",
    "\n",
    "The functions $\\mathbf{s}_{\\theta_1}(\\cdot)$, $\\mathbf{t}_{\\theta_2}(\\cdot)$, and `scale_factor` are already implemented in the provided class `CouplingLayer` below for your convinience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9j430iTQ5qYQ",
    "tags": []
   },
   "source": [
    "<img src=\"https://img.icons8.com/offices/80/000000/making-notes.png\" style=\"height:30px;display:inline\\\">**<span style=\"color:red\">Task 6</span>**. Implement missing `forward` and `inverse` methods in the class `CouplingLayer`:\n",
    "  * The `forward` method should take in `x` and use the mask `self.mask` and the learnable functions `self.s_func`, `self.scale_factor` and `self.t_func` to predict the transformation parameters and compute `y`. This method should also return the parameter `log_det_jac` which is the log-determinant of the Jacobian.\n",
    "  * The `inverse` method goes in the other direction. It takes in `y` and uses `self.mask`, `self.s_func`, `self.scale_factor` and `self.t_func` to compute `x`. This method should also return the parameter `inv_log_det_jac` which is the log-determinant of the Jacobian of $f^{-1}(\\cdot)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlnxwkUpUacJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CouplingLayer(nn.Module):\n",
    "  def __init__(self, mask):\n",
    "    super(CouplingLayer, self).__init__()\n",
    "\n",
    "    # mask for splitting (fixed not learnable)\n",
    "    self.mask = nn.Parameter(mask, requires_grad=False)\n",
    "\n",
    "    # scaling function and stabilizing scale_factor init. to 0\n",
    "    self.s_func = nn.Sequential(nn.Linear(in_features=2, out_features=32), \n",
    "                                nn.LeakyReLU(),\n",
    "                                nn.Linear(in_features=32, out_features=32), \n",
    "                                nn.LeakyReLU(),\n",
    "                                nn.Linear(in_features=32, out_features=2))\n",
    "    self.scale_factor = nn.Parameter(torch.Tensor(2).fill_(0.0))\n",
    "\n",
    "    # shifting function\n",
    "    self.t_func = nn.Sequential(nn.Linear(in_features=2, out_features=32), \n",
    "                                nn.LeakyReLU(),\n",
    "                                nn.Linear(in_features=32, out_features=32), \n",
    "                                nn.LeakyReLU(),\n",
    "                                nn.Linear(in_features=32, out_features=2))\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    TODO: replace y and log_det_jac with your code.\n",
    "    \"\"\"\n",
    "    x_part_1 = torch.mul(x, self.mask)\n",
    "    inv_mask = 1 - self.mask\n",
    "    x_part_2 = torch.mul(x, inv_mask)\n",
    "    s = self.s_func(x_part_1)\n",
    "    t = self.t_func(x_part_1) * inv_mask\n",
    "    scale_s = torch.mul(s, self.scale_factor)\n",
    "    exp_scale = torch.exp(scale_s)\n",
    "    y_part_1 = x_part_1\n",
    "    y_part_2 = torch.mul(torch.mul(exp_scale, x_part_2) + t, inv_mask)\n",
    "    y = y_part_1 + y_part_2\n",
    "    log_det_jac = torch.sum(scale_s*inv_mask,dim=1)\n",
    "    return y, log_det_jac\n",
    "\n",
    "  def inverse(self, y):\n",
    "    \"\"\"\n",
    "    TODO: replace x and inv_log_det_jac with your code.\n",
    "    \"\"\"\n",
    "    y_part_1 = torch.mul(y,self.mask)\n",
    "    inv_mask = 1-self.mask\n",
    "    y_part_2 = torch.mul(y,inv_mask)\n",
    "    s = self.s_func(y_part_1)\n",
    "    t = self.t_func(y_part_1)\n",
    "    exp_scale = torch.exp(-torch.mul(s,self.scale_factor))\n",
    "    x_part_2 = torch.mul((y_part_2 - t),exp_scale)\n",
    "    x_part_1 = y_part_1\n",
    "\n",
    "    x = torch.concat((x_part_1[:,torch.where(self.mask==1)],x_part_2[:,torch.where(inv_mask==1)]),dim=1)\n",
    "    inv_log_det_jac = -torch.sum(s)\n",
    "    return x, inv_log_det_jac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test my implementation\n",
    "mask = torch.tensor([1.,0.])\n",
    "l = CouplingLayer(mask)\n",
    "x = torch.tensor([[1.,2.],[3,4]])\n",
    "y, jac = l(x)\n",
    "print('y:' ,y, 'J:', jac)\n",
    "#test inverse:\n",
    "x_revert, jac_inv = l.inverse(y)\n",
    "print('x:' ,x_revert, 'J inv :' ,jac_inv)\n",
    "\n",
    "assert torch.allclose(x,x_revert), \"x input and output aren't the same!\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0pXhQCBdYS8"
   },
   "source": [
    "### Coupling Flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kq9LVwtb5ttq",
    "tags": []
   },
   "source": [
    "As you might have guessed by now, a coupling layer is powerful yet still limited in its ability to significantly alter the input. This is because it only operates on a chunk of it with element-wise manipulations due to the invertability constraint. We can go on with making our function $f$ more complex. How can we implement more complex invertible functions? The answer is: invertible function composition. We can stack multiple invertible functions $f_1,\\dots,f_K$ (e.g. Coupling Layers) after each other, as all together, they still represent a single, invertible function. Specifically, if $\\mathbf{y} = f_1(\\mathbf{z})$ and $\\mathbf{x} = f_2(\\mathbf{y})$ are invertible functions, then $\\mathbf{x} = f_2 \\circ f_1 (\\mathbf{z})$ is an invertible function and its inverse is given by $f_1^{-1} \\circ f_2^{-1}$. More importantly, the calculation of the log-determinant of the Jacobian in this case is simple using the chain rule.\n",
    "\n",
    "<img src=\"https://img.icons8.com/offices/80/000000/making-notes.png\" style=\"height:30px;display:inline\\\">**<span style=\"color:red\">Task 7</span>**. Assuming $\\mathbf{y} = f_1(\\mathbf{z})$ and $\\mathbf{x} = f_2(\\mathbf{y})$ are coupling layers, calculate the log-determinant of the Jacobian $\\frac{d \\mathbf{x}}{d \\mathbf{z}}$. How is it related to the log-determinant of the Jacobians $\\frac{d \\mathbf{y}}{d \\mathbf{z}}$ and $\\frac{d \\mathbf{x}}{d \\mathbf{y}}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7\n",
    "$$ \\log |J_{f_1,f_2}| = \\log(|\\frac{dx}{dy}\\frac{dy}{dz}|)= \\log(|\\frac{dx}{dy}||\\frac{dy}{dz}|)= \\log(|\\frac{dx}{dy}|+ \\log |\\frac{dy}{dz}|) = \\sum_n{s(y_{1:j})_n} + \\sum_n{s(z_{1:j})_n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkeUwDVKBzjb"
   },
   "source": [
    "Coupling layers generalize to any masking technique we could think of. However, the most common approach is to split the input $\\mathbf{x}$ in half using the mask. For our toy 2D datasets comprised of samples $\\mathbf{x} = (p_x ,p_y) \\in \\mathbb{R}^{d=2}$, this means that either $\\left\\{\\mathbf{x}_{1:j}=p_x, \\mathbf{x}_{j+1:d}=p_y\\right\\}$, or $\\left\\{\\mathbf{x}_{1:j}=p_y, \\mathbf{x}_{j+1:d}=p_x\\right\\}$. These correspond to masks $\\left[1,0\\right]^T$ and $\\left[0,1\\right]^T$ respectively. \n",
    "Note that when we apply multiple coupling layers, we invert the masking every other layer so that each variable is transformed a similar amount of times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhdG1tam73Gy"
   },
   "source": [
    "#### Intuition in 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGhcD03b7b2x"
   },
   "source": [
    "Intuitively, using multiple, learnable invertible functions, a normalizing flow attempts to transform $p_z(z)$ slowly into a more complex distribution which should finally be $p_x(x)$. We visualize the idea below\n",
    "(figure credit - [Lilian Weng](https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html)):\n",
    "\n",
    "<center width=\"100%\"><img src=\"https://lilianweng.github.io/posts/2018-10-13-flow-models/normalizing-flow.png\" width=\"700px\"></center>\n",
    "\n",
    "Starting from $z_0$, which follows the prior Gaussian distribution, we sequentially apply the invertible functions $f_1,f_2,...,f_K$, until $z_K$ represents $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xh1jgKYS7skC"
   },
   "source": [
    "#### Implementation\n",
    "\n",
    "Using the `CouplingLayer` class from above, here we will implement a class named `CouplingFlow` that is comprised of stacked coupling layers. This class will have the following attributes:\n",
    "* `num_layers` - a scalar passed at initialization that will determine the number of coupling layers to stack, refered to later as $K$.\n",
    "* `self.layers` - a Module list comprised of $K$ stacked coupling layers each with its own mask. Note that the masks are fixed and non-learnable therefore we set their `requires_grad` property to `False` inside `CouplingLayer`.\n",
    "* `self.prior` - This is the prior/base distribution. Here we will use a standard Gaussian distribution with a unit variance per dimension implemented using the `torch.distributions` module.\n",
    "\n",
    "<img src=\"https://img.icons8.com/offices/80/000000/making-notes.png\" style=\"height:30px;display:inline\\\">**<span style=\"color:red\">Task 8</span>**. Implement the following 3 methods of this class:\n",
    "* `log_probability` - method that takes in a batch of samples `x` and returns `log_prob` which is their log-probability $\\log p(\\mathbf{x})$. For convinience we will assume the overall function $f=f_K\\circ f_{K-1} \\dots \\circ f_1$ satisfies $\\mathbf{x} = f(\\mathbf{z})$. Hence, to calculate the log-probability you should employ the inverse functions $f_i^{-1}$ starting from the last layer $f_K^{-1}$ (assuming $K$ layers).\n",
    "* `sample_x` - method that takes in a parameter `num_samples` and returns samples `x` from $p(\\mathbf{x})$ alongside their log-probability values `log_prob`. \n",
    "* `sample_x_each_step` - method takes in a parameter `num_samples` and returns a list of samples `samples` after each intermediate coupling layer in `self.layers`. The first element of this list is samples from the prior distribution $p(\\mathbf{z})$ and the last element is samples from $p(\\mathbf{x})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qtGxIE0wWUM0"
   },
   "outputs": [],
   "source": [
    "class CouplingFlow(nn.Module):\n",
    "  def __init__(self, num_layers):\n",
    "      super(CouplingFlow, self).__init__()\n",
    "\n",
    "      # concatenate coupling layers with alternating masks\n",
    "      masks = F.one_hot(torch.tensor([i % 2 for i in range(num_layers)])).float()\n",
    "      self.layers = nn.ModuleList([CouplingLayer(mask) for mask in masks])\n",
    "\n",
    "      # define prior distribution to be z~N(0,I)\n",
    "      self.prior = MultivariateNormal(torch.zeros(2),torch.eye(2))\n",
    "\n",
    "  def log_probability(self, x):\n",
    "    \"\"\"\n",
    "    TODO: replace log_prob with your code.\n",
    "    \"\"\"\n",
    "    log_det_jac = 0\n",
    "    num_of_layers = len(self.layers)\n",
    "    for i, _ in enumerate(self.layers):\n",
    "        x , log_det_jac_i = self.layers[i](x)\n",
    "        log_det_jac+=log_det_jac_i\n",
    "    y=x # switch to y for convinience\n",
    "    log_p_y= self.prior.log_prob(y)\n",
    "    log_prob= log_p_y + log_det_jac\n",
    "    return log_prob\n",
    "\n",
    "  def sample_x(self, num_samples):\n",
    "    \"\"\"\n",
    "    TODO: replace x and log_prob with your code.\n",
    "    \"\"\"\n",
    "    y = self.prior.sample([num_samples])\n",
    "    num_of_layers = len(self.layers)\n",
    "    inv_log_det_jac = 0\n",
    "    for i, _ in enumerate(self.layers):\n",
    "        y , inv_log_det_jac_i = self.layers[num_of_layers-i-1].inverse(y)\n",
    "        inv_log_det_jac+=inv_log_det_jac_i\n",
    "    x = y\n",
    "    log_p_y = self.prior.log_prob(y)\n",
    "    log_prob = log_p_y + inv_log_det_jac # +inv_log_det_jac_i\n",
    "    return x, log_prob\n",
    "\n",
    "  def sample_x_each_step(self, num_samples):\n",
    "    \"\"\"\n",
    "    TODO: replace samples with your code.\n",
    "    \"\"\"\n",
    "    log_det_jac = 0\n",
    "    samples = []\n",
    "    y = self.prior.sample([num_samples])\n",
    "    num_of_layers = len(self.layers)\n",
    "    inv_log_det_jac = 0\n",
    "    samples.append(y)\n",
    "    for i, _ in enumerate(self.layers):\n",
    "        y , inv_log_det_jac_i = self.layers[num_of_layers-i-1].inverse(y)\n",
    "        samples.append(y)\n",
    "        #inv_log_det_jac+=inv_log_det_jac_i\n",
    "        #log_p_y = self.prior.log_prob(y)\n",
    "        #log_prob = log_p_y + inv_log_det_jac # +inv_log_det_jac_i\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test my imp.\n",
    "cf = CouplingFlow(2)\n",
    "x = torch.tensor([[1,2],[3,4]])\n",
    "lp = cf.log_probability(x)\n",
    "print('log(p(x)):',lp.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,log_prob = cf.sample_x(10)\n",
    "print('Samples from x (2d): \\n',x.data, '\\nlog prob:' , log_prob.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = cf.sample_x_each_step(2)\n",
    "print([sample.data for sample in samples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsi46N-pGbTd"
   },
   "source": [
    "### Training Coupling Flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAqTOzgT5yDH"
   },
   "source": [
    "Now that we have finished implementing the flow model, we can start training it. Provided below is an already implemented function `train` to train your `CouplingFlow` models. The function recieves 4 arguments:\n",
    "* `model` - an instance of the class `CouplingFlow`.\n",
    "* `data` - an instance of the class `ToyDataset`.\n",
    "* `epochs` - number of epochs to train the model (int). \n",
    "* `batch_size` - the batch size to use in training (int)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUD7LIqsE_Jk"
   },
   "outputs": [],
   "source": [
    "# detach tensor and transfer to numpy\n",
    "def to_np(x):\n",
    "  return x.detach().numpy()\n",
    "\n",
    "# Simple training function\n",
    "def train(model, data, epochs = 100, batch_size = 64):\n",
    "\n",
    "  # move model into the device\n",
    "  model = model.to(device)\n",
    "\n",
    "  # split into training and validation, and create the loaders\n",
    "  lengths = [int(len(data)*0.9), len(data) - int(len(data)*0.9)]\n",
    "  train_set, valid_set = random_split(data, lengths)\n",
    "  train_loader = DataLoader(train_set, batch_size=batch_size)\n",
    "  valid_loader = DataLoader(valid_set, batch_size=batch_size)\n",
    "\n",
    "  # define the optimizer and scheduler\n",
    "  optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "  # train the model\n",
    "  train_losses, valid_losses, min_valid_loss = [], [], np.Inf\n",
    "  with tqdm.tqdm(range(epochs), unit=' Epoch') as tepoch:\n",
    "    for epoch in tepoch:\n",
    "\n",
    "      # training loop\n",
    "      epoch_loss = 0\n",
    "      model.train(True)\n",
    "      for batch_index, training_sample in enumerate(train_loader):\n",
    "          log_prob = model.log_probability(training_sample)\n",
    "          loss = - log_prob.mean(0)\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          epoch_loss += loss\n",
    "      epoch_loss /= len(train_loader)\n",
    "      train_losses.append(np.copy(to_np(epoch_loss)))\n",
    "\n",
    "      # validation loop\n",
    "      epoch_loss_valid = 0\n",
    "      model.train(False)\n",
    "      for batch_index, valid_sample in enumerate(valid_loader):\n",
    "        log_prob = model.log_probability(valid_sample)\n",
    "        loss_valid = - log_prob.mean(0)\n",
    "        epoch_loss_valid += loss_valid\n",
    "      \n",
    "      epoch_loss_valid /= len(valid_loader)\n",
    "      valid_losses.append(np.copy(to_np(epoch_loss_valid)))\n",
    "      \n",
    "      # save best model based off validation loss\n",
    "      if epoch_loss_valid < min_valid_loss:\n",
    "        model_best = copy.deepcopy(model)\n",
    "        min_valid_loss = epoch_loss_valid\n",
    "        epoch_min = epoch\n",
    "\n",
    "      # report progress with tqdm pbar\n",
    "      tepoch.set_postfix(train_loss=to_np(epoch_loss), valid_loss=to_np(epoch_loss_valid))\n",
    "      \n",
    "  # report best model on val.\n",
    "  print('\\n Best Model achieved {:.4f} validation loss at epoch {} \\n'.\n",
    "        format(min_valid_loss, epoch_min))\n",
    "\n",
    "  # if the number of samples is too low take the final weights regardless of \n",
    "  # valdiation loss due to weak statistics (overfitting avoided by early stopping)\n",
    "  if lengths[1] < 500:\n",
    "    model_best = model\n",
    "\n",
    "  return model_best, train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zX1SvGdBsih5"
   },
   "source": [
    "Here is an example snippet for using this function to train a flow model with $K=4$ layers on a dataset of $N=1500$ samples $\\mathbf{x}_i$ from the `'Moons'` distribution for 1000 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aycd27V7tSz6",
    "outputId": "99ae49cb-1a83-4c25-de7a-6e23c670714a"
   },
   "outputs": [],
   "source": [
    "# seeds to ensure reproducibility\n",
    "torch.manual_seed(8)\n",
    "np.random.seed(0)\n",
    "\n",
    "# dataset\n",
    "num_samples = 1500\n",
    "data = ToyDataset('Moons', num_samples=num_samples)\n",
    "\n",
    "# learning hyper-parameters\n",
    "K = 4\n",
    "nepochs = 1000\n",
    "\n",
    "# instantiate model and optimize the parameters\n",
    "Flow_model = CouplingFlow(num_layers=K)\n",
    "moon_model, train_loss, valid_loss = train(Flow_model, data, epochs=nepochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7v4dR5an6b4w"
   },
   "source": [
    "<img src=\"https://img.icons8.com/offices/80/000000/making-notes.png\" style=\"height:30px;display:inline\\\">**<span style=\"color:red\">Task 9</span>**. For each of the 4 provided distributions (use the default value of `num_gaussians=5` for `'GaussiansRot'`), use a similar snippet to repeat the following:\n",
    "  * Create a `ToyDataset` instance with $N=1500$ samples from the distribution.\n",
    "  * Learn a `CouplingFlow` model with $K=4$ layers, by training for 500 epochs. For the `'Moons'` dataset train for 1000 epochs.\n",
    "  * Plot the estimated density $p(\\mathbf{x})$ in $\\mathbb{R}^2$. To achieve this, use the method `log_probability` to calculate the log-probability $\\log p(\\mathbf{x})$ at a pre-determined grid of points $\\mathbf{x} \\in \\left[x_{\\min},x_{\\max}\\right]\\times\\left[y_{\\min},y_{\\max}\\right]\\subset\\mathbb{R}^2$ (e.g. using `np.meshgrid`). Sample each coordinate with at least 100 points (i.e. a grid of $100 \\times 100$ positions in 2D). Plot the resulting 2D distribution $p(\\mathbf{x})=\\exp (\\log p(\\mathbf{x}))$ as an image where the value in each pixel is $p(\\mathbf{x})$.\n",
    "  * Plot samples from intermediate flow layers, including the prior $p(\\mathbf{z})$ and the modelled $p(\\mathbf{x})$. To achieve this, use the method `sample_x_each_layer` with $N=1000$ samples. Plot the resulting samples from each layer in the **same** axis limits to visualize the transformation of each coupling layer separately. You can use `plt.subplot(..., sharex=True, sharey=True)` to achieve link the axis of all subplots.\n",
    "\n",
    "\n",
    "Implementation tips:\n",
    "* Use the same seeds as the example snippet for reproducibility. This will also ensure a non-diverging erroneous behavior with other seeds.\n",
    "* To make sure the model is not overfitting you can look at the training and the valdiation loss outputs of the provided function `train`. Do not include these in your report, use them just for sanity check.\n",
    "* Estimate the log-probability in a reasonable vicinity of the training domain. For far away coordinates from the training data, the estimation could be poor locally and might bias the dynamic range of your plot.\n",
    "* To avoid code duplication, you are encouraged to implement two plotting functions: one for plotting the density, and one for plotting the transformations across layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_dist_on_grid(title,model,xmin,xmax,ymin,ymax):\n",
    "        g = torch.meshgrid(torch.linspace(xmin,xmax,100),torch.linspace(ymin,ymax,100))\n",
    "        grid = torch.concat((g[0],g[1]),dim=0).reshape(2, -1).T\n",
    "        log_p = model.log_probability(grid)\n",
    "        log_p_mesh = (log_p.T).reshape(g[0].shape[0],g[0].shape[1])\n",
    "        log_p_mesh_np = log_p_mesh.detach().numpy()\n",
    "        p = np.exp(log_p_mesh_np)\n",
    "        plt.figure(figsize = (5,5))\n",
    "        plt.imshow(p,extent =[xmin,\n",
    "                             xmax,\n",
    "                             ymin,\n",
    "                             ymax])\n",
    "        plt.title(title)\n",
    "        plt.savefig(title +'.png')\n",
    "\n",
    "def train_dist(dist, N=1500,nepochs=500,K=4,num_gaussians=5):\n",
    "    torch.manual_seed(8)\n",
    "    np.random.seed(0)\n",
    "    num_samples = N\n",
    "    data = ToyDataset(dist, num_samples=num_samples, num_gaussians=num_gaussians)\n",
    "    # instantiate model and optimize the parameters\\\n",
    "    Flow_model = CouplingFlow(num_layers=K)\n",
    "    model, train_loss, valid_loss = train(Flow_model, data, epochs=nepochs)\n",
    "    return model,train_loss,valid_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "circles_model, train_loss, valid_loss = train_dist('Circles',nepochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_model_dist_on_grid('Circles',circles_model,-1,1,-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GaussiansGrid_model, train_loss, valid_loss = train_dist('GaussiansGrid',nepochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_dist_on_grid('GaussiansGrid',GaussiansGrid_model,-2,4,-2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "GaussiansRot_model, train_loss, valid_loss = train_dist('GaussiansRot',nepochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_dist_on_grid('GaussiansRot',GaussiansRot_model,-4,4,-4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Moon_model ,train_loss, valid_loss = train_dist('Moons',nepochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_model_dist_on_grid('Moons',Moon_model,-1,2,-1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_intermidiate_dists(title,model,num_samples,k=4):\n",
    "    samples = model.sample_x_each_step(num_samples)\n",
    "    titles = ['$p(z_0)$','$p(z_1)$','$p(z_2)$','$p(z_3)$','$p(x)$']\n",
    "    numofaxes=5\n",
    "    if (k==2):\n",
    "        titles = ['$p(z_0)$','$p(z_1)$','$p(x)$']\n",
    "        numofaxes=3\n",
    "    fig,axes = plt.subplots(1,numofaxes,sharex=True,sharey=True,figsize=[20,4])\n",
    "    fig.suptitle(title)\n",
    "    for i, (sample, ax) in enumerate(zip(samples,axes)):\n",
    "        sample = torch.detach(sample).numpy()\n",
    "        ax.scatter(sample[:,0],sample[:,1],s=5,alpha=0.3)\n",
    "        ax.set_title(titles[i])\n",
    "    plt.gcf().set_dpi(300)\n",
    "    plt.savefig(title +'.png')\n",
    "\n",
    "dists = ['Circles', 'Moons', 'GaussiansGrid', 'GaussiansRot']\n",
    "models = [circles_model, Moon_model,GaussiansGrid_model,GaussiansRot_model]\n",
    "num_samples=1000\n",
    "for model,dist in zip(models,dists):\n",
    "    title = dist + ' samples per layer'\n",
    "    plot_intermidiate_dists(title,model,num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jmMeBhXMDdq"
   },
   "source": [
    "#### Analyzing Coupling Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZiVjNxHOGxh"
   },
   "source": [
    "<img src=\"https://img.icons8.com/offices/80/000000/making-notes.png\" style=\"height:30px;display:inline\\\">**<span style=\"color:red\">Task 10</span>**. Train two flow models with a varying number of coupling layers $K=\\left\\{2, 4\\right\\}$ for 250 epochs, using $N=1500$ samples from the `'GaussiansRot'` dataset with `num_gaussians=5`. Compare the resulting estimated density $p(\\mathbf{x})$ (using a grid of $100 \\times 100$ points) and the intermediate distributions of $N=1000$ samples throughout the coupling layers of the model. Which model fits $p(\\mathbf{x})$ better? What do you conclude regarding the effect of model depth? explain the result in your report and attach the resulting plots.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = 'GaussiansRot'\n",
    "k=2\n",
    "nepochs=250\n",
    "N=1500\n",
    "model_k2 ,train_loss, valid_loss = train_dist(dist,nepochs=nepochs,K=k,N=N)\n",
    "title = dist + ' 2 Layers'\n",
    "plot_model_dist_on_grid(title,model_k2,-4,4,-4,4)\n",
    "plot_intermidiate_dists(title +' samples per layer',model_k2,num_samples=1000,k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=4\n",
    "nepochs=250\n",
    "N=1500\n",
    "model_k4 ,train_loss, valid_loss = train_dist(dist,nepochs=nepochs,K=k,N=N)\n",
    "title = dist + ' 4 Layers'\n",
    "plot_model_dist_on_grid(title,model_k4,-4,4,-4,4)\n",
    "plot_intermidiate_dists(title+' samples per layer',model_k4,num_samples=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xj6DedgfQyqJ"
   },
   "source": [
    "<img src=\"https://img.icons8.com/offices/80/000000/making-notes.png\" style=\"height:30px;display:inline\\\">**<span style=\"color:red\">Task 11</span>**. Train two flow models with $K=4$ layers for 400 epochs, using a varying number of $N=\\left\\{1500, 3000\\right\\}$ samples from the `'GaussiansRot'` dataset with `num_gaussians=5`. Compare the resulting estimated density $p(\\mathbf{x})$ (using a grid of $100 \\times 100$ points) and the intermediate distributions of $N=1000$ samples throughout the coupling layers of the model. Which model fits $p(\\mathbf{x})$ better? What do you conclude regarding the effect of training set size? explain the result in your report and attach the resulting plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = 'GaussiansRot'\n",
    "n=1500\n",
    "nepochs=400\n",
    "K=4\n",
    "model_n1500 ,train_loss, valid_loss = train_dist(dist,nepochs=nepochs,K=K,N=n)\n",
    "title = dist + ' 1500 samples'\n",
    "plot_model_dist_on_grid(title,model_n1500,-4,4,-4,4)\n",
    "plot_intermidiate_dists(title+' samples per layer',model_n1500,num_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = 'GaussiansRot'\n",
    "n=3000\n",
    "K=4\n",
    "nepochs= 400\n",
    "model_n3000 ,train_loss, valid_loss = train_dist(dist,nepochs=nepochs,K=K,N=n)\n",
    "title = dist + ' 3000 samples'\n",
    "plot_model_dist_on_grid(title,model_n3000,-4,4,-4,4)\n",
    "plot_intermidiate_dists(title+' samples per layer',model_n3000,num_samples=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XanpzXL_R-Nk"
   },
   "source": [
    "<img src=\"https://img.icons8.com/offices/80/000000/making-notes.png\" style=\"height:30px;display:inline\\\">**<span style=\"color:red\">Task 12</span>**. Train two flow models with $K=4$ layers for 250 epochs, using $N=1500$ samples from the `'GaussiansRot'` dataset with a varying number of Gaussians `num_gaussians`$=\\left\\{3, 7\\right\\}$. Compare the resulting estimated density $p(\\mathbf{x})$ (using a grid of $100 \\times 100$ points) and the intermediate distributions of $N=1000$ samples throughout the coupling layers of the model. Which distribution $p(\\mathbf{x})$ is fitted better by the model? What do you conclude regarding the effect of data complexity? explain the result in your report and attach the resulting plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = 'GaussiansRot'\n",
    "N=1500\n",
    "K=4\n",
    "num_gaussians=3\n",
    "nepochs=250\n",
    "model_3gaussians ,train_loss, valid_loss = train_dist(dist,nepochs=nepochs,K=K,N=N,num_gaussians=num_gaussians)\n",
    "title = dist + ' 3 gaussians'\n",
    "plot_model_dist_on_grid(title,model_3gaussians,-4,4,-4,4)\n",
    "plot_intermidiate_dists(title+' samples per layer',model_3gaussians,num_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = 'GaussiansRot'\n",
    "num_gaussians=7\n",
    "model_7gaussians ,train_loss, valid_loss = train_dist(dist,nepochs=nepochs,K=K,N=N,num_gaussians=num_gaussians)\n",
    "title = dist + ' 7 gaussians'\n",
    "plot_model_dist_on_grid(title,model_7gaussians,-4,4,-4,4)\n",
    "plot_intermidiate_dists(title+' samples per layer',model_7gaussians,num_samples=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okAzepD9Cgmi"
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "In conclusion, we have seen how to implement our own normalizing flow on toy 2D datasets. However, as mentioned in the beginning of Part II, similar models with significantly more layers and additional tricks can be used to model images (e.g. [Glow](http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions.pdf)). The most common flow element, the coupling layer, is simple to implement, and yet effective. Normalizing flows are an interesting generative model compared to GANs, as they allow an exact likelihood estimate in continuous space, and we have the guarantee that every possible input $x$ has a corresponding latent vector $z$. Recent advances in [Neural ODEs](https://arxiv.org/pdf/1806.07366.pdf) allow a flow with infinite number of layers, called Continuous Normalizing Flows, whose potential is yet to fully explore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elP89YCFCgmi"
   },
   "source": [
    "## References and Credits\n",
    "\n",
    "[1] Dinh, L., Sohl-Dickstein, J., and Bengio, S. (2017). “Density estimation using Real NVP,” In: 5th International Conference on Learning Representations, ICLR 2017. [Link](https://arxiv.org/abs/1605.08803)\n",
    "\n",
    "[2] Kingma, D. P., and Dhariwal, P. (2018). “Glow: Generative Flow with Invertible 1x1 Convolutions,” In: Advances in Neural Information Processing Systems, vol. 31, pp. 10215--10224. [Link](http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions.pdf)\n",
    "\n",
    "[3] University of Amsterdam, Deep Learning 1, Tutorial 11. [Link](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial11/NF_image_modeling.html#Normalizing-Flows-as-generative-model)\n",
    "\n",
    "[4] Technical University of Munich, Machine Learning for Graphs and Sequential Data, Generative Models. [Link](https://www.in.tum.de/en/daml/teaching/summer-term-2020/machine-learning-for-graphs-and-sequential-data/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW1_Spring2022_Solved_Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
