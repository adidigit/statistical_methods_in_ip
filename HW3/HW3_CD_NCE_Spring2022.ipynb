{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXrAM0RgCgl8"
   },
   "source": [
    "# Statistical Methods in Image Processing EE-048954\n",
    "## Homework 3: Contrastive Divergence and Noise Contrastive Estimation\n",
    "### Due Date: <span style=\"color:red\">June 16, 2022</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Submission Guidelines\n",
    "\n",
    "* Submission only in **pairs** on the course website (Moodle).\n",
    "* Working environment:\n",
    "    * We encourage you to work in `Jupyter Notebook` online using <a href=\"https://colab.research.google.com/\">Google Colab</a> as it does not require any installation.\n",
    "* You should submit two **separated** files:\n",
    "    * A `.ipynb` file, with the name: `ee048954_hw3_id1_id2.ipynb` which contains your code implementations.\n",
    "    * A `.pdf` file, with the name: `ee048954_hw3_id1_id2.pdf` which is your report containing plots, answers, and discussions.\n",
    "    * **No handwritten submissions** and no other file-types (`.docx`, `.html`, ...) will be accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXYdlXNS4c6V"
   },
   "source": [
    "### Mounting your drive for saving/loading stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VtmwdpfGMmjD",
    "outputId": "bd97d2b8-531f-486b-c5f7-8ccab95baec3"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXPsd-7TgsCN"
   },
   "source": [
    "### Importing relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XFz780qygx3J"
   },
   "outputs": [],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "\n",
    "## Scipy optimization routines\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "## Progress bar\n",
    "import tqdm\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BxH6tQygjiN"
   },
   "source": [
    "## Part I: Contrastive Divergence (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following Gaussian Mixture Model (GMM) distribution\n",
    "\n",
    "$$ p(x;\\{\\mu_i\\}) = \\sum_{i=1}^{N}\\frac{1}{N}\\,\\frac{1}{{{2\\pi}}} \\exp\\left\\{-\\frac{1}{2}||x-\\mu_i||^2\\right\\} ,$$\t\n",
    "where $x,\\mu_i \\in \\mathbb{R}^2$. We will use $N = 4$, $\\sigma = 1$, and $\\{\\mu_i\\} = \\{(0,0)^T , (0,3)^T , (3,0)^T , (3,3)^T\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling from GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img.icons8.com/offices/80/000000/making-notes.png\" style=\"height:30px;display:inline\\\">**<span style=\"color:red\">Task 1</span>**. Direct sampling: Use your function from HW1 that accepts $\\{\\mu_i\\}$, and returns a sample $x$ from $p(x;\\{\\mu_i\\})$. Draw $J=1000$ samples $\\{x\\}$ from the distribution $p(x;\\{\\mu_i\\})$ using this function. These will be our **real samples**.\n",
    "\n",
    "<img src=\"https://img.icons8.com/offices/80/000000/making-notes.png\" style=\"height:30px;display:inline\\\">**<span style=\"color:red\">Task 2</span>**. Sampling with MCMC: implement the MALA algorithm to draw samples from $p(x;\\{\\mu_i\\})$. The function will get an initial guess $\\{\\hat x_i\\}$ and will generate chains of length $L$. Use $\\sqrt{2\\varepsilon} = 0.1$ and $N \\sim \\mathcal{N}(0,I)$.\n",
    "\n",
    "**From now on**, we will refer to **$\\{\\mu_i\\}$ as unknowns** and we will estimate them using different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 1\n",
    "from scipy.stats import multivariate_normal as mv_normal\n",
    "\n",
    "def sample_from_gaussian_mixture(M,sigma,miu_m,N):\n",
    "    \"\"\"\n",
    "    Sample Gaussian Mixture\n",
    "    :param M:\n",
    "    :param sigma:\n",
    "    :param miu_m:\n",
    "    :param N:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    gauss_components = np.random.choice(M, N ,replace=True)# sample N gaussian components\n",
    "    xs= np.array([np.random.multivariate_normal(miu,sigma) for miu in miu_m[gauss_components]])\n",
    "    pdf = 0\n",
    "    for miu in miu_m:\n",
    "        pdf+=mv_normal.pdf(xs,miu,sigma)\n",
    "    return xs,pdf/M\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=4\n",
    "sigma=np.eye(2)\n",
    "miu_m = np.array([[0,0],[0,3],[3,0],[3,3]])\n",
    "J=1000\n",
    "real_samples = sample_from_gaussian_mixture(N,sigma,miu_m,J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 2\n",
    "# task 8\n",
    "\n",
    "def log_prob_grad(x,mus,sigma_2):\n",
    "    sum_num = 0\n",
    "    sum_denom = 0\n",
    "    for mu in mus:\n",
    "        exp_arg = torch.exp(-(2*sigma_2)**(-1)*torch.norm(x-mu,dim=1)**2)\n",
    "        sum_num += ((x-mu).T* exp_arg)\n",
    "        sum_denom +=exp_arg\n",
    "        \n",
    "    grad = -(sigma_2)**(-1)*sum_num/sum_denom\n",
    "    return grad\n",
    "    \n",
    "    \n",
    "def q_x_prime_given_x(x_prime,x,grad_log_x,eps):\n",
    "    q = torch.exp(-1/(4*eps)*torch.norm(x_prime-x-eps*grad_log_x)**2)\n",
    "    return q\n",
    "\n",
    "def MALA_update(x_k,p_x_k,grad_x_k,sqrt_two_eps):\n",
    "    ### Check acceptance criteria:\n",
    "    norm = normal.MultivariateNormal(torch.zeros(image_size,image_size),torch.eye(image_size))\n",
    "    samples = norm.sample((n_imgs,1)).to(device)\n",
    "    x_k_plus_1 = x_k + ((sqrt_two_eps**2)/2)*grad_x_k + sqrt_two_eps * samples\n",
    "    p_x_k_plus_1 = ebm(x_k_plus_1)\n",
    "    grad_x_k_plus_1 = -torch.autograd.grad(p_x_k_plus_1.sum(), x_k_plus_1)[0]\n",
    "\n",
    "    is_accepted = p_x_k_plus_1>p_x_k\n",
    "    only_accepted_x_k_plus_1 = x_k_plus_1[is_accepted]\n",
    "    not_accepted_x_k_plus_1 = x_k_plus_1[~is_accepted]\n",
    "    \n",
    "    ### replace x_k_plus_1 with x_k with probability alpha:\n",
    "    if len(not_accepted_x_k_plus_1)>0:\n",
    "        q_x_k_given_x_k_plus_1 = q_x_prime_given_x(x_k,x_k_plus_1,grad_x_k_plus_1,eps)\n",
    "        q_x_k_plus_1_given_x_k = q_x_prime_given_x(x_k_plus_1,x_k,grad_x_k,eps)\n",
    "        alpha = p_x_k_plus_1*q_x_k_given_x_k_plus_1/(p_x_k*q_x_k_plus_1_given_x_k)\n",
    "\n",
    "        alpha_not_accepted = alpha[~is_accepted]\n",
    "        u = torch.rand(len(not_accepted_x_k_plus_1)).to(device)\n",
    "        is_accepted_after_MALA = u<=alpha_not_accepted\n",
    "        accepted_after_MALA_x_k_plus_1 = not_accepted_x_k_plus_1[is_accepted_after_MALA]\n",
    "        x_k_remain_after_MALA = (x_k[~is_accepted])[~is_accepted_after_MALA]\n",
    "        #print(len(accepted_after_MALA_x_k_plus_1) ,' were accepted after MALA')\n",
    "        #not_accepted_after_MALA = not_accepted[~is_accepted_after_MALA]\n",
    "        x_k_plus_1_final = torch.concat((only_accepted_x_k_plus_1,accepted_after_MALA_x_k_plus_1,x_k_remain_after_MALA))\n",
    "    else:\n",
    "        #print('all accepted')\n",
    "        x_k_plus_1_final = x_k_plus_1 # all samples got updated.\n",
    "        \n",
    "    return x_k_plus_1_final\n",
    "\n",
    "def pdf_gaussian_mixture(x,mus,sigma):\n",
    "    pdf = 0\n",
    "    for mu in mus:\n",
    "        pdf+=mv_normal.pdf(x,mu,sigma)\n",
    "    return pdf/len(mus)\n",
    "                                       \n",
    "# number of images to generate\n",
    "def ld_with_MALA(L,sqrt_two_eps,init_samples,mus,sigma):\n",
    "    \"\"\"\n",
    "    Sample with MALA\n",
    "    :L: chain length\n",
    "    :sqrt_two_eps:\n",
    "    :init_samples: initial guess\n",
    "    :mus: expectation of gaussian mixture\n",
    "    :sigma: std of gaussian mixture\n",
    "    \"\"\"\n",
    "    x_k = init_samples\n",
    "    eps = (sqrt_two_eps**2)/2\n",
    "    # norm = normal.MultivariateNormal(torch.zeros(image_size,image_size),torch.eye(image_size))\n",
    "    p_x_k = pdf_gaussian_mixture(x_k,mus,sigma)\n",
    "    grad_x_k = log_prob_grad(x_k,mus,sigma)\n",
    "    \n",
    "    for l in range(L):\n",
    "        # run the model: input the images x, getting as output their estimated energy E(x)\n",
    "        x_k= MALA_update(x_k,p_x_k,\n",
    "                         grad_x_k,\n",
    "                         sqrt_two_eps\n",
    "                        )\n",
    "        p_x_k = pdf_gaussian_mixture(x_k,mus,sigma)\n",
    "        grad_x_k = log_prob_grad(x_k,mus,sigma)\n",
    "\n",
    "        if (k%200 == 0):\n",
    "           print('iteration {0} completed'.format(k))\n",
    "        #_all_x.append(x_k[0])\n",
    "    return x_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt_two_eps= 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimation of $\\{\\mu_i\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img.icons8.com/offices/80/000000/making-notes.png\" style=\"height:30px;display:inline\\\">**<span style=\"color:red\">Task 3</span>**. Implement Maximum likelihood (ML) estimation of $\\{\\mu_i\\}$ using direct sampling:\n",
    "* Step 1: Randomly initialize $\\{\\tilde \\mu_i\\}$ from $U([0,3]^2)$.\n",
    "* Step 2: Use your function from Task 1 to draw 100 samples ${\\tilde x}$ from $p(x;\\{\\mu_i\\})$ using $\\{\\tilde \\mu_i\\}$.\n",
    "* Step 3: Update $\\{\\tilde \\mu_i\\}$ using the ML gradient descent step:\n",
    "$$ \\tilde \\mu_i ^ {k+1} = \\tilde \\mu_i ^ {k} + \\eta\\left(\\langle \\nabla_{\\mu_i} \\log p(x;\\{\\mu_i\\})\\rangle_{x}-\\langle \\nabla_{\\mu_i} \\log p(x;\\{\\mu_i\\})\\rangle_{\\tilde x}\\right),$$\n",
    "where $\\langle\\cdot\\rangle_x$ denotes averaging over the real samples from Task 1 and $\\langle\\cdot\\rangle_{\\tilde x}$ denotes averaging over the synthetically generated samples from Step 2. Use $\\eta = 1$.\n",
    "* Repeat Step 2 and Step 3 until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img.icons8.com/offices/80/000000/making-notes.png\" style=\"height:30px;display:inline\\\">**<span style=\"color:red\">Task 4</span>**. Implement Maximum likelihood (ML) estimation of $\\{\\mu_i\\}$ using MCMC:\n",
    "* Step 1: Randomly initialize $\\{\\tilde \\mu_i\\}$ from $U([0,3]^2)$.\n",
    "* Step 2: Use your function from Task 2 to draw 100 samples ${\\tilde x}$ from $p(x;\\{\\mu_i\\})$ using $\\{\\tilde \\mu_i\\}$. Initialize the chains with $\\hat x_i \\sim \\mathcal{N}(1.5,2)$ and use chains length of $L=1000$.\n",
    "* Step 3: Update $\\{\\tilde \\mu_i\\}$ using the ML gradient descent step:\n",
    "$$ \\tilde \\mu_i ^ {k+1} = \\tilde \\mu_i ^ {k} + \\eta\\left(\\langle \\nabla_{\\mu_i} \\log p(x;\\{\\mu_i\\})\\rangle_{x}-\\langle \\nabla_{\\mu_i} \\log p(x;\\{\\mu_i\\})\\rangle_{\\tilde x}\\right),$$\n",
    "where $\\langle\\cdot\\rangle_x$ denotes averaging over the real samples from Task 1 and $\\langle\\cdot\\rangle_{\\tilde x}$ denotes averaging over the synthetically generated samples from Step 2. Use $\\eta = 1$.\n",
    "* Repeat Step 2 and Step 3 until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img.icons8.com/offices/80/000000/making-notes.png\" style=\"height:30px;display:inline\\\">**<span style=\"color:red\">Task 5</span>**. Implement Contrastive Divergence (CD) estimation of $\\{\\mu_i\\}$ using MCMC sampling:\n",
    "* Step 1: Randomly initialize $\\{\\tilde \\mu_i\\}$ from $U([0,3]^2)$.\n",
    "* Step 2: Use your function from Task 2 to draw 100 samples ${\\tilde x}$ from $p(x;\\{\\mu_i\\})$ using $\\{\\tilde \\mu_i\\}$. Initialize the chains with **100 samples randomly chosen from the real set of examples from Task 1**, and use only $L=10$ update steps.\n",
    "* Step 3: Update $\\{\\tilde \\mu_i\\}$ using the CD gradient descent step:\n",
    "$$ \\tilde \\mu_i ^ {k+1} = \\tilde \\mu_i ^ {k} + \\eta\\left(\\langle \\nabla_{\\mu_i} \\log p(x;\\{\\mu_i\\})\\rangle_{x}-\\langle \\nabla_{\\mu_i} \\log p(x;\\{\\mu_i\\})\\rangle_{\\tilde x}\\right),$$\n",
    "where $\\langle\\cdot\\rangle_x$ denotes averaging over the $100$ real samples used for initialization of the chains in Step 2 and $\\langle\\cdot\\rangle_{\\tilde x}$ denotes averaging over the MCMC generated samples from Step 3. Use $\\eta = 1$.\n",
    "* Repeat Step 2 and Step 3 until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img.icons8.com/offices/80/000000/making-notes.png\" style=\"height:30px;display:inline\\\">**<span style=\"color:red\">Task 6</span>**.\n",
    "Present the estimated $\\{\\mu_i\\}$ and the final random samples $\\{\\tilde x_i\\}$ generated with each of the three algorithms in Tasks 3-5. Discuss the differences in convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BxH6tQygjiN"
   },
   "source": [
    "## Part II: Noise Contrastive Estimation (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the distribution\n",
    "$$p_m(x;\\{\\mu_i\\}) = \\frac{1}{Z} \\sum_{i=1}^{N} \\exp\\left\\{-\\frac{1}{2\\sigma^2}||x-\\mu_i||^2\\right\\} ,$$\t\n",
    "where $Z \\in \\mathbb{R}$ is a normalization constant, and $ x,\\mu_i \\in \\mathbb{R}^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling from GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img.icons8.com/offices/80/000000/making-notes.png\" style=\"height:30px;display:inline\\\">**<span style=\"color:red\">Task 7</span>**. What is the value of $Z$?\n",
    "\n",
    "<img src=\"https://img.icons8.com/offices/80/000000/making-notes.png\" style=\"height:30px;display:inline\\\">**<span style=\"color:red\">Task 8</span>**. Use $N = 4$, $\\sigma = 1$, and $\\{\\mu_i\\} = \\{(0,0)^T , (0,3)^T , (3,0)^T , (3,3)^T\\} $. Draw $J=1000$ samples $\\{x_j\\}$ from the distribution $p_m(x;\\{\\mu_i\\})$ using the function from Task 1.\n",
    "\n",
    "**From now on**, we will refer to **$\\{\\mu_i\\}$ as unknowns** and we will estimate them using the Noise Contrastive Estimation method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimation of $\\{\\mu_i\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img.icons8.com/offices/80/000000/making-notes.png\" style=\"height:30px;display:inline\\\">**<span style=\"color:red\">Task 9</span>**. Implement Noise Contrastive Estimation of $\\{\\mu_i\\}$:\n",
    "* Step 1: Generating the artificial data-set of noise: Draw $J=1000$ samples $\\{y_j\\}$ from $$p_n(y;\\mu_n) = \\frac{1}{{{2\\pi \\sigma_n^2}}} \\exp\\left\\{-\\frac{1}{2\\sigma_n^2}||y-\\mu_n||^2\\right\\}$$\n",
    "  using $\\mu_n = (1,1)^T$ and $\\sigma_n=2$.\n",
    "* Step 2: Randomly select an initial guess for the model means $\\{\\tilde \\mu_i\\}$ from $U([0,3]^2)$.\n",
    "* Step 3: Update $\\{\\tilde \\mu_i\\}$ by **maximizing**:\n",
    "$$\\{\\tilde \\mu_i\\} = \\underset{\\{\\mu_i\\}}{\\arg\\max} \\, \\sum_{j=1}^{J} \\left[\\ln (h(x_j;\\{\\mu_i\\})) + \\ln(1-h(y_j;\\{\\mu_i\\})) \\right],$$\n",
    "where\n",
    "$$h(u;\\{\\mu_i\\}) = \\frac{p_m(u;\\{\\mu_i\\})}{p_m(u;\\{\\mu_i\\})+p_n(u;\\mu_n)}.$$\n",
    "  Implementation Tip: This step can be executed using the function `scipy.optimize.minimize` which finds the **minimum** of an (unconstrained) optimization problem (e.g. using the `'BFGS'` method), given a function that calculates the objective and an initial guess (see scipy documentation for more details). In our case, for **maximization**, implement a function that calculates the **minus** of the objective above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now regard both $\\{\\mu_i\\}$ **and the normalization constant $Z$** as unknowns, and will estimate them using Noise Contrastive Estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img.icons8.com/offices/80/000000/making-notes.png\" style=\"height:30px;display:inline\\\">**<span style=\"color:red\">Task 10</span>**. Implement Noise Contrastive Estimation with an **un-normalized** probability model:\n",
    "* Step 1: Generating the artificial data-set of noise: Draw $J=1000$ samples $\\{y_j\\}$ from $$p_n(y;\\mu_n) = \\frac{1}{{{2\\pi \\sigma_n^2}}} \\exp\\left\\{-\\frac{1}{2\\sigma_n^2}||y-\\mu_n||^2\\right\\}$$\n",
    "  using $\\mu_n = (1,1)^T$ and $\\sigma_n=2$.\n",
    "* Step 2: Randomly select an initial guess for the model means $\\{\\tilde \\mu_i\\}$ from $U([0,3]^2)$, and for the normalization constant $Z$ from $U([0.1,1])$\n",
    "* Step 3: Update $\\{\\tilde \\mu_i\\}$ and $Z$ by **maximizing**:\n",
    "$$\\{\\tilde \\mu_i\\}, Z = \\underset{\\{\\mu_i\\}, Z}{\\arg\\max} \\, \\sum_{j=1}^{J} \\left[\\ln (h(x_j;\\{\\mu_i\\}, Z)) + \\ln(1-h(y_j;\\{\\mu_i\\}, Z)) \\right],$$\n",
    "where\n",
    "$$h(u;\\{\\mu_i\\}, Z) = \\frac{p_m(u;\\{\\mu_i\\}, Z)}{p_m(u;\\{\\mu_i\\}, Z)+p_n(u;\\mu_n)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img.icons8.com/offices/80/000000/making-notes.png\" style=\"height:30px;display:inline\\\">**<span style=\"color:red\">Task 11</span>**. Visually: plot the estimates of $\\{\\mu_i\\}$ of Tasks 9 and 10 (two separate figures). Include the model samples, the noise samples, the initial guess for the model means, and the final estimates of $\\{\\tilde \\mu_i\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img.icons8.com/offices/80/000000/making-notes.png\" style=\"height:30px;display:inline\\\">**<span style=\"color:red\">Task 12</span>**. Quantitatively: repeat Tasks 9 and 10, this time with $J = 100\\times[1,5,10,20,30,50]$. For each value of $J$ repeat the estimation process for 50 times, each time with different realizations for $\\{x_j\\}$ and $\\{y_j\\}$ and initial guesses for the estimands ($\\{\\mu_i\\}$ in Task 9 and $\\{\\mu_i\\},Z$ in Task 10. \n",
    "\n",
    "For each value of $J$, calculate the MSE between the true parameter values and their estimates (the mean will be taken over the different realizations). Note that for the model means, the MSE should be calculated to the closest true $\\mu_i$ for each estimation. If at the same run two estimated $\\mu_i$s pick the same true $\\mu_i$, then this run should be declared as a failure and should be disregarded. Report the number of failure runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img.icons8.com/offices/80/000000/making-notes.png\" style=\"height:30px;display:inline\\\">**<span style=\"color:red\">Task 13</span>**. Discussion: How does the number of samples $J$ affect the accuracy of the estimation? How does the addition of $Z$ as an unknown affect the accuracy? "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW1_Spring2022_Solved_Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
